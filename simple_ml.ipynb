{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating simple artificial intelligence in Python\n",
    "\n",
    "In this tutorial, we will be classifying what type of Iris a set of values are. Is it setosa, versicolor, or virginica? Using sklearn, we will be doing this automatically, and hopefully be able to predict a certain species of flower *even if we don't know specifically what the species is*. \n",
    "\n",
    "#### Step 1: Data Preparation\n",
    "First, we need to load our data from a file. We need to separate them into two arrays: \n",
    "\n",
    "- Training Array (often denoted as \"X\")\n",
    "    - Your training array will contain a set of values. In our example, we will be passing the sepal length and width, and petal length and width.\n",
    "- Target Array (often denoted as \"y\")\n",
    "    - Your target array will contain the *answers* to the given training array.\n",
    "\n",
    "For simplicity, I will be using the following English vs. German example:\n",
    "\n",
    "|Training Array|Target Array|\n",
    "|-----|-----|\n",
    "|ANYONE|English|\n",
    "|UPROAR|English|\n",
    "|YELLOW|English|\n",
    "|BÄRGET|German|\n",
    "|ZURUFE|German|\n",
    "|WÜSTEM|German|\n",
    "\n",
    "Unlike the above example, we are going to be using Iris data instead. First, let's create our training array. We shall be using the traditional way of loading data in Python, in contrast to `pandas`, for reference and simplicity.\n",
    "\n",
    "##### Traditionally loading files in Python\n",
    "You can open files natively, without the need of libraries, using the `open()` function. `open()` returns a file-like object. In this case, it returns a TextIOWrapper object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = open(\"iris-setosa.csv\", 'r')\n",
    "print(type(my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read a TextIOWrapper object, you can use `read()` or `readlines()`.\n",
    "\n",
    "Here, we will use `readlines()`. Readlines returns a list of strings, with each element being the corresponding line within the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_line in my_file:\n",
    "    print(text_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using what we learnt about glob from yesterday, we can go ahead and load all of the files in a loop. Instead of using pandas, however, we will be using the same method that we used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity: Read multiple files using glob, and append all values into a list\n",
    "\n",
    "So, using the notebook that we used yesterday, go ahead and create a cell that will load all `*.csv`s into an array and append each line into a *singular list*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a string list with all of the species above. However, we need to convert this into a *number* list. \n",
    "\n",
    "Machine Learning, in the barest form, is Mathematics. It's impossible for us to do math with words, and thus it's imperative to change your data into numbers.\n",
    "\n",
    "For example, if you were training AI on English words, you may end up converting your word into a list of numbers:\n",
    "\n",
    "|English|To Numbers|\n",
    "|-----|-----|\n",
    "|hello|`[8, 5, 12, 12, 15]`\n",
    "\n",
    "Then, you would train your AI on the list of numbers, and your AI will spit out another list of numbers. You would then have to re-translate them back to English.\n",
    "\n",
    "|Numbers|To English|\n",
    "|-----|-----|\n",
    "|`[19, 8, 9, 6, 20]`|shift|\n",
    "\n",
    "So in this scenario, we have a row that looks like\n",
    "\n",
    "`\"5.0,3.3,1.4,0.2,Iris-setosa\"`\n",
    "\n",
    "We need to convert this string into a list, so that we get the specific values, similar to:\n",
    "\n",
    "`[5.0, 3.3, 1.4, 0.2, \"Iris-setosa\"]`\n",
    "\n",
    "We can split strings and turn them into arrays really easily using the `split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"5.0,3.3,1.4,0.2,Iris-setosa\"\n",
    "string_to_array = \"5.0,3.3,1.4,0.2,Iris-setosa\".split(\",\")\n",
    "\n",
    "print(string_to_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an array, but our number values are still strings (which is not helpful). We need to convert them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 4)\n",
      "[5.0, 3.3, 1.4, 0.2, 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "print(range(len(string_to_array) - 1))\n",
    "\n",
    "for item in range(len(string_to_array) - 1):\n",
    "    string_to_array[item] = float(string_to_array[item])\n",
    "    \n",
    "print(string_to_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've converted our values properly, but we have one problem: 'Iris-setosa' needs to become a number. How should we do this? Remember: this is 1 out of the 3 species included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activity: Do our solution for all of the lines within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "training = []\n",
    "target = []\n",
    "label_dict = {}\n",
    "\n",
    "for item in glob.glob(\"iris*.csv\"):\n",
    "    lines = open(item, 'r').readlines()\n",
    "    del lines[0]\n",
    "    for line in lines:\n",
    "        training_slice = line.split(\",\")[:-1]\n",
    "        slice_to_float = [float(i) for i in training_slice]\n",
    "        training.append(slice_to_float)\n",
    "        label = line.split(\",\")[-1].replace(\"\\n\", '')\n",
    "        try:\n",
    "            target.append(label_dict[label])\n",
    "        except KeyError:\n",
    "            label_dict[label] = len(label_dict)\n",
    "            target.append(label_dict[label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we have both of our training and target arrays prepared. Now, let's do some simple machine learning!\n",
    "\n",
    "First, we need to import certain scripts/functions from the `sklearn` library, which stands for SciKit Learn. We can do that using the `from ... import ...` statement.\n",
    "\n",
    "The above allows you to import only *specific* things within the library, instead of the entire thing. This can help in terms of memory management and performance of your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sklearn` is incredibly easy, and creating a neural network can be done in two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dax\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the MLPClassifier\n",
    "mlp_nn = MLPClassifier()\n",
    "\n",
    "# Fit (or train) the MLPClassifier with the training\n",
    "# and corresponding target data.\n",
    "mlp_nn.fit(training, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_nn.predict([training[145]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we're missing certain values in our dataset (such as NaN values). Now, it may be easy to take the mean of a given column for a particular species and put them in the rows that are missing values.\n",
    "\n",
    "However, many times it is a lot better and more accurate to use a neural network to *predict* values that are missing. Let's look at that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.559,None,2.047,0.787,Iris-virginica\n",
      "\n",
      "2.441,None,2.126,0.906,Iris-virginica\n",
      "\n",
      "2.323,None,2.008,0.709,Iris-VirGinICA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_v = open('missing-iris-virginica.csv', 'r')\n",
    "\n",
    "for item in missing_v.readlines()[-3:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as usual, let's make the training and target lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.48, 2.362, 0.984],\n",
       " [2.283, 2.008, 0.748],\n",
       " [2.795, 2.323, 0.827],\n",
       " [2.48, 2.205, 0.709],\n",
       " [2.559, 2.283, 0.866],\n",
       " [2.992, 2.598, 0.827],\n",
       " [1.929, 1.772, 0.669],\n",
       " [2.874, 2.48, 0.709],\n",
       " [2.638, 2.283, 0.709],\n",
       " [2.835, 2.402, 0.984],\n",
       " [2.559, 2.008, 0.787],\n",
       " [2.52, 2.087, 0.748],\n",
       " [2.677, 2.165, 0.827],\n",
       " [2.244, 1.969, 0.787],\n",
       " [2.283, 2.008, 0.945],\n",
       " [2.52, 2.087, 0.906],\n",
       " [2.559, 2.165, 0.709],\n",
       " [3.031, 2.638, 0.866],\n",
       " [3.031, 2.717, 0.906],\n",
       " [2.362, 1.969, 0.591],\n",
       " [2.717, 2.244, 0.906],\n",
       " [2.205, 1.929, 0.787],\n",
       " [3.031, 2.638, 0.787],\n",
       " [2.48, 1.929, 0.709],\n",
       " [2.638, 2.244, 0.827],\n",
       " [2.835, 2.362, 0.709],\n",
       " [2.441, 1.89, 0.709],\n",
       " [2.402, 1.929, 0.709],\n",
       " [2.52, 2.205, 0.827],\n",
       " [2.835, 2.283, 0.63],\n",
       " [2.913, 2.402, 0.748],\n",
       " [3.11, 2.52, 0.787],\n",
       " [2.52, 2.205, 0.866],\n",
       " [2.48, 2.008, 0.591],\n",
       " [2.402, 2.205, 0.551],\n",
       " [3.031, 2.402, 0.906],\n",
       " [2.48, 2.205, 0.945],\n",
       " [2.52, 2.165, 0.709],\n",
       " [2.362, 1.89, 0.709],\n",
       " [2.717, 2.126, 0.827],\n",
       " [2.638, 2.205, 0.945],\n",
       " [2.717, 2.008, 0.906],\n",
       " [2.283, 2.008, 0.748],\n",
       " [2.677, 2.323, 0.906],\n",
       " [2.638, 2.244, 0.984],\n",
       " [2.638, 2.047, 0.906],\n",
       " [2.48, 1.969, 0.748]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.299, 1.063, 1.181, 1.142, 1.181, 1.181, 0.984, 1.142, 0.984, 1.417, 1.26, 1.063, 1.181, 0.984, 1.102, 1.26, 1.181, 1.496, 1.024, 0.866, 1.26, 1.102, 1.102, 1.063, 1.299, 1.26, 1.102, 1.181, 1.102, 1.181, 1.102, 1.496, 1.102, 1.102, 1.024, 1.181, 1.339, 1.22, 1.181, 1.22, 1.22, 1.22, 1.063, 1.26, 1.299, 1.181, 0.984]\n"
     ]
    }
   ],
   "source": [
    "missing_v = open('missing-iris-virginica.csv', 'r')\n",
    "trainingm_list = []\n",
    "targetm_list = []\n",
    "\n",
    "for item in missing_v.readlines()[1:-3]:\n",
    "    this_row = []\n",
    "    for idx, value in enumerate(item.split(\",\")[:-1]):\n",
    "        if idx == 1:\n",
    "            targetm_list.append(float(value))\n",
    "        else:\n",
    "            this_row.append(float(value))\n",
    "    trainingm_list.append(this_row)\n",
    "    \n",
    "display(trainingm_list)\n",
    "print(targetm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, let's recreate the neural network. Instead of a classifier, we need a regressor.\n",
    "\n",
    "The simplest difference between a classifier and a regressor is that a classifier predicts *categories* or discrete numbers. A regressor predicts *numerical values* or continuous functions.\n",
    "\n",
    "We're not trying to *classify* the data, we're trying to predict the best potential *numerical value* for the missing data. Thus, we will import a different neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=672,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create the MLPClassifier\n",
    "m_mlp_nn = MLPRegressor(random_state=672)\n",
    "\n",
    "# Fit (or train) the MLPClassifier with the training\n",
    "# and corresponding target data.\n",
    "m_mlp_nn.fit(trainingm_list, targetm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a script to take the missing values from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.559, 2.047, 0.787], [2.441, 2.126, 0.906], [2.323, 2.008, 0.709]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_v = open('missing-iris-virginica.csv', 'r')\n",
    "missing_values_list = []\n",
    "\n",
    "for item in missing_v.readlines()[-3:]:\n",
    "    this_row = []\n",
    "    for idx, value in enumerate(item.split(\",\")[:-1]):\n",
    "        if value != 'None':\n",
    "            this_row.append(float(value))\n",
    "    missing_values_list.append(this_row)\n",
    "    \n",
    "display(missing_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our missing values. Let's use the created neural network in order to predict these missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.20175054, 1.30719757, 1.19051255])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_values = m_mlp_nn.predict(missing_values_list)\n",
    "display(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the predicted values for our missing data. Let's see how well the neural network did given the original values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.181], [1.339], [1.181]]\n"
     ]
    }
   ],
   "source": [
    "original_values = open('iris-virginica.csv', 'r')\n",
    "original_values_list = []\n",
    "\n",
    "for item in original_values.readlines()[-3:]:\n",
    "    this_row = []\n",
    "    for idx, value in enumerate(item.split(\",\")[:-1]):\n",
    "        if idx == 1:\n",
    "            this_row.append(float(value))\n",
    "    original_values_list.append(this_row)\n",
    "\n",
    "print(original_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems do have done quite alright! If we used the mean of the values, for example, we would've gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of sepal_width column is 1.1667446808510633\n"
     ]
    }
   ],
   "source": [
    "missing_v = open('missing-iris-virginica.csv', 'r')\n",
    "sepal_width_list = []\n",
    "\n",
    "for item in missing_v.readlines()[1:-3]:\n",
    "    for idx, value in enumerate(item.split(\",\")[:-1]):\n",
    "        if idx == 1:\n",
    "            sepal_width_list.append(float(value))\n",
    "    \n",
    "mean = sum(sepal_width_list) / len(sepal_width_list)\n",
    "print(f\"The mean of sepal_width column is {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
