{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, from scratch.\n",
    "\n",
    "## Table of Contents (clickable):\n",
    "1. [A long, easy to follow version with comments walking you through the code](#Long-version-with-comments)\n",
    "2. [A long, easy to follow version without comments](#Long-version-without-comments)\n",
    "3. [A short, hard to follow version for people comfortable with programming and interested to know some of what Python is capable of.](Concise-version-of-the-code)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long version with comments\n",
    "#### Creating a language classifier\n",
    "\n",
    "Let's develop a plan:\n",
    "- First, import necessary libraries\n",
    "- Second, load the data into the correct format\n",
    "    - Segregating out data into \"training\" and target arrays\n",
    "    - Changing all data into some numerical format interpretable by a machine\n",
    "- Third, instantiate some classifier and let the classifier \"learn\" the data.\n",
    "\n",
    "##### Importing libraries\n",
    "For this task, the only library we would actually need to import is ```sklearn```'s ```MLPClassifier```. Potentially, if you need to load a lot of data, you may want to import ```glob```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a multilayer perceptron from scikit-learn\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the data\n",
    "There are multiple different things we need to take into account:\n",
    "- All of the data will need to be converted into some numerical format. As such, words may be translated into some numerical form, etc.\n",
    "- Most AI algorithms and NN (unless specified otherwise or being built yourself), need fixed-size samples. In this specific scenario of a language classifier, this means that all of the words need to be of the same length (all 4 letter words, or all 6 letter words, etc.)\n",
    "- We will need to segregate the data into \"training\" and \"target\" samples. Training samples contain the data/features that you would like to predict on, and target samples contain the answer to the training sample. As such, \"thanks\" is a training sample with target \"English,\" and \"Kamusta\" is a training sample with target \"Filipino.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying which file names to load\n",
    "files = [\"english.txt\", \"german.txt\"]\n",
    "\n",
    "# Creating both the training and target lists.\n",
    "training = []\n",
    "target = []\n",
    "\n",
    "# Opening each filename within the list \"files\" in a for loop. This for loop runs twice: \n",
    "# once for english.txt and another for german.txt.\n",
    "for file in files:\n",
    "    \n",
    "    # Opening the file using the function open()\n",
    "    opened_file = open(file, 'r', encoding='latin-1')\n",
    "    \n",
    "    # Reading the lines within the file using the function readlines()\n",
    "    file_lines = opened_file.readlines()\n",
    "    \n",
    "    # We are trying to determine whether or not the language is English \n",
    "    # so we can append the 0 to the target array (0 being our numerical representation of English)\n",
    "    # There are better, more automated ways of doing this.\n",
    "    if file == \"english.txt\":\n",
    "        \n",
    "        # To fix the fixed-size sample problem, we will only take six-letter words from the dataset\n",
    "        # We are going to make a counter that checks how many six-letter words we encounter\n",
    "        six_letter_words = 0\n",
    "        \n",
    "        # We are going through each and every line within the file. As we know, each line in the\n",
    "        # file actually corresponds to a word. Therefore, this can be seen as looking through \n",
    "        # each and every word within the file.\n",
    "        for line in file_lines:\n",
    "            \n",
    "            # If we look at each \"word,\" we see that it ends with an ugly \"\\n\" character, which\n",
    "            # is the new-line character. We'll need to remove it as we only want the real letters,\n",
    "            # not the characters that denote a new-line to outputs or text editors.\n",
    "            # We are using the replace() method in order to replace \"\\n\" with \"\", the empty\n",
    "            # character, effectively deleting it.\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            \n",
    "            # We can further normalize the words by making them all lower-case\n",
    "            line = line.lower()\n",
    "            \n",
    "            # Now, let's check if the line/word is of length 6.\n",
    "            if len(line) == 6:\n",
    "                \n",
    "                # If it is of length six, make a list where each element within that list is the\n",
    "                # number representation of the letter. The ord() function takes a character and \n",
    "                # makes a numerical representation to it. chr() is the opposite - it takes a number\n",
    "                # and translates it back to a character.\n",
    "                orded_line = []\n",
    "                for letter in line:\n",
    "                    orded_line.append(ord(letter))\n",
    "                    \n",
    "                # Now, lets add that number representation of the word to our training array\n",
    "                training.append(orded_line)\n",
    "                \n",
    "                # Then, lets add 1 to the variable six_letter_words in order to keep track of how\n",
    "                # many answers we need to make.\n",
    "                six_letter_words += 1\n",
    "        \n",
    "        # We need to make the answers for each of our six letter words now. Using the \n",
    "        # six_letter words variable, we know how many 0s we need for each of the \n",
    "        # training samples. Remember that the number 0 is how we're telling the machine that\n",
    "        # the training sample is English.\n",
    "        class_target = []\n",
    "        for _ in range(six_letter_words):\n",
    "            class_target.append(0)\n",
    "        \n",
    "        # Last, lets concatenate our created list above of 0s to our main target (answer) list.\n",
    "        # We would need to do this as we are storing values of other things as well, such as 1\n",
    "        # for German.\n",
    "        target += class_target\n",
    "    else:\n",
    "        # Notice that this is part of the if-statement where the file != english.txt. Therefore,\n",
    "        # we are accessing the German file.\n",
    "        # Everything here is functionally the same as the above, but we are instead appending\n",
    "        # 1 in our target array as we want these to represent German.\n",
    "        six_letter_words = 0\n",
    "        for line in file_lines:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            line = line.lower()\n",
    "            if len(line) == 6:\n",
    "                orded_line = []\n",
    "                for letter in line:\n",
    "                    orded_line.append(ord(letter))\n",
    "                training.append(orded_line)\n",
    "                six_letter_words += 1\n",
    "                \n",
    "        class_target = []\n",
    "        for _ in range(six_letter_words):\n",
    "            class_target.append(1)\n",
    "            \n",
    "        target += class_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a pre-defined neural network and using it on our data\n",
    "\n",
    "Now that we've created our data, let's go ahead and train a pre-defined multilayer perceptron and use it for predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll need to instantiate our neural network. We can do this pretty easily by setting a variable\n",
    "# to MLPClassifier().\n",
    "mlp_nn = MLPClassifier()\n",
    "\n",
    "# To train, we use the fit() function. We give it the training and target lists that we have made\n",
    "# earlier.\n",
    "mlp_nn.fit(training, target)\n",
    "\n",
    "# Once that is finished training, you can go ahead and make some predictions using predict(). \n",
    "# Again, the same rules apply, you would need to translate them into numerical interpretations,\n",
    "# and that the data will need to be six-letter words as above.\n",
    "\n",
    "# Here are two words that don't really exist, but are 1-letter modifications to existing\n",
    "# words\n",
    "english_word = \"hellow\"\n",
    "german_word = \"flüche\"\n",
    "\n",
    "orded_eng = []\n",
    "for letter in english_word:\n",
    "    orded_eng.append(ord(letter))\n",
    "    \n",
    "orded_ger = []\n",
    "for letter in german_word:\n",
    "    orded_ger.append(ord(letter))\n",
    "    \n",
    "# You need to format your prediction samples in a n+1 dimensional list for neural networks other than \n",
    "# MLP. In this scenario, we are using MLP and can only train on and predict 2-dimensional lists. \n",
    "# An easy way to think about list dimensionality is that 1-dimensional does not contain a list\n",
    "# inside of a list. Two dimensional is a list in a list. Three dimensional is a list in a list in a list,\n",
    "# etc. Our word to number representation is a one dimensional list: [0, 0, 0, 0, 0, 0]. Our training\n",
    "# and target data is a two-dimensional list as it is a list of our word to number representations. \n",
    "# [[0, 0, 0, 0, 0, 0], where each of these lines is another word within our training array.\n",
    "#  [0, 0, 0, 0, 0, 0],\n",
    "#  [0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "# If you want to only predict one sample:\n",
    "mlp_nn.predict([[0, 0, 0, 0, 0, 0]])\n",
    "mlp_nn.predict([orded_eng]) # orded_eng is already a list as seen above, and it is within another brackets\n",
    "                            # to denote that it is within a different list, making it two dimensional.\n",
    "\n",
    "# If you want to predict more than one, just add more lists to the outermost list. Here we show two samples\n",
    "mlp_nn.predict([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]])\n",
    "mlp_nn.predict([orded_eng, orded_ger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long version without comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [\"english.txt\", \"german.txt\"]\n",
    "\n",
    "training = []\n",
    "target = []\n",
    "\n",
    "for file in files:\n",
    "    opened_file = open(file, 'r', encoding='latin-1')\n",
    "    file_lines = opened_file.readlines()\n",
    "\n",
    "    if file == \"english.txt\":\n",
    "        \n",
    "        six_letter_words = 0\n",
    "        for line in file_lines:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            line = line.lower()\n",
    "            \n",
    "            if len(line) == 6:\n",
    "                orded_line = []\n",
    "                for letter in line:\n",
    "                    orded_line.append(ord(letter))\n",
    "                training.append(orded_line)\n",
    "                six_letter_words += 1\n",
    "        \n",
    "        class_target = []\n",
    "        for _ in range(six_letter_words):\n",
    "            class_target.append(0)\n",
    "        \n",
    "        target += class_target\n",
    "    else:\n",
    "        six_letter_words = 0\n",
    "        for line in file_lines:\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            line = line.lower()\n",
    "            if len(line) == 6:\n",
    "                orded_line = []\n",
    "                for letter in line:\n",
    "                    orded_line.append(ord(letter))\n",
    "                training.append(orded_line)\n",
    "                six_letter_words += 1\n",
    "                \n",
    "        class_target = []\n",
    "        for _ in range(six_letter_words):\n",
    "            class_target.append(1)\n",
    "            \n",
    "        target += class_target\n",
    "\n",
    "mlp_nn = MLPClassifier()\n",
    "mlp_nn.fit(training, target)\n",
    "\n",
    "english_word = \"hellow\"\n",
    "german_word = \"flüche\"\n",
    "\n",
    "orded_eng = []\n",
    "for letter in english_word:\n",
    "    orded_eng.append(ord(letter))\n",
    "    \n",
    "orded_ger = []\n",
    "for letter in german_word:\n",
    "    orded_ger.append(ord(letter))\n",
    "    \n",
    "mlp_nn.predict([orded_eng, orded_ger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concise version of the code\n",
    "For people comfortable with programming, here is a version of the code that exemplifies how short Python can be, and given enough knowledge of Python, how many more explicit versions of code in other languages (e.g. Java) can be shorted by taking advantage of Python features such as generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "training = []\n",
    "target = []\n",
    "for idx, file in enumerate([\"english.txt\", \"german.txt\"]):\n",
    "    lines = open(file).readlines()\n",
    "    sample = [[ord(c) for c in line.replace(\"\\n\", \"\").lower()] for line in lines if len(line) == 7]\n",
    "    training += sample\n",
    "    target += [idx for _ in range(len(sample))]\n",
    "\n",
    "mlp_nn = MLPClassifier()\n",
    "mlp_nn.fit(training, target)\n",
    "\n",
    "mlp_nn.predict([[ord(c) for c in word] for word in [\"hellow\", \"flüche\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
